# ğŸ¤– Traductor Transformer: EspaÃ±ol a InglÃ©s ğŸ“–

## âœ¨ Â¡Bienvenido a este proyecto educativo!

Este repositorio contiene el cÃ³digo para un modelo de **Traductor AutomÃ¡tico Neuronal** que utiliza la arquitectura **Transformer**. El objetivo principal es desglosar y explicar, de una manera clara y concisa, cÃ³mo funciona esta tecnologÃ­a que ha revolucionado el campo del Procesamiento del Lenguaje Natural (NLP).

El modelo ha sido entrenado para traducir oraciones del **espaÃ±ol al inglÃ©s**. 

---

## ğŸ›ï¸ La Arquitectura Transformer: Una Mirada Detallada

El modelo Transformer, introducido en el famoso paper "[Attention Is All You Need](https://arxiv.org/abs/1706.03762)", se basa en un mecanismo llamado **atenciÃ³n** para entender las relaciones entre palabras en una oraciÃ³n. A diferencia de modelos anteriores (como las RNNs), no procesa las palabras secuencialmente, lo que le permite ser mucho mÃ¡s rÃ¡pido y eficiente.

Nuestro modelo se compone de dos partes principales: el **Codificador (Encoder)** y el **Decodificador (Decoder)**.

### ğŸ§  El Codificador (Encoder)

La misiÃ³n del codificador es "entender" la oraciÃ³n en el idioma original (espaÃ±ol). Para ello, la procesa a travÃ©s de una pila de capas idÃ©nticas. Cada capa tiene dos sub-componentes clave:

1.  **AtenciÃ³n Multi-Cabeza (Multi-Head Attention)** ğŸ¯
    * **Â¿QuÃ© hace?** Permite que el modelo, al procesar una palabra, preste atenciÃ³n a todas las demÃ¡s palabras de la oraciÃ³n.
    * **Â¿Por quÃ© "Multi-Cabeza"?** En lugar de hacerlo una sola vez, realiza este cÃ¡lculo de atenciÃ³n varias veces en paralelo (8 en nuestro caso). Cada "cabeza" se especializa en detectar diferentes tipos de relaciones (sintÃ¡cticas, semÃ¡nticas, etc.). Es como tener varios expertos analizando la oraciÃ³n desde distintos Ã¡ngulos a la vez.

2.  **Red Feed-Forward (Position-wise Feed-Forward Network)** ğŸ’¡
    * **Â¿QuÃ© hace?** Una vez que la capa de atenciÃ³n ha generado una representaciÃ³n de la oraciÃ³n rica en contexto, esta red neuronal procesa cada posiciÃ³n de forma independiente. Ayuda a transformar y refinar la informaciÃ³n obtenida por la atenciÃ³n.

### âœï¸ El Decodificador (Decoder)

El decodificador tiene la tarea de generar la traducciÃ³n palabra por palabra en el idioma de destino (inglÃ©s), utilizando la informaciÃ³n que le proporciona el codificador. TambiÃ©n tiene una pila de capas, pero cada una de ellas tiene **tres** sub-componentes:

1.  **AtenciÃ³n Multi-Cabeza Enmascarada (Masked Multi-Head Attention)** ğŸ­
    * **Â¿QuÃ© hace?** Similar a la del codificador, pero con una "mÃ¡scara". Al predecir la siguiente palabra de la traducciÃ³n, el modelo solo puede prestar atenciÃ³n a las palabras que ya ha generado, no a las futuras. Â¡Esto evita que haga trampa mirando la respuesta!

2.  **AtenciÃ³n Codificador-Decodificador (Encoder-Decoder Attention)** ğŸ¤
    * **Â¿QuÃ© hace?** Â¡AquÃ­ ocurre la magia de la traducciÃ³n! Esta capa permite que el decodificador preste atenciÃ³n a las palabras mÃ¡s relevantes de la oraciÃ³n original en espaÃ±ol para generar la siguiente palabra en inglÃ©s. Por ejemplo, al traducir "gato", prestarÃ¡ mucha atenciÃ³n a la palabra "cat" en la entrada.

3.  **Red Feed-Forward (Position-wise Feed-Forward Network)** ğŸ’¡
    * **Â¿QuÃ© hace?** Al igual que en el codificador, procesa y refina la salida de las capas de atenciÃ³n para prepararla para la siguiente capa o para la salida final.

### ğŸ“ CodificaciÃ³n Posicional (Positional Encoding)

Dado que el Transformer no procesa las palabras en orden, Â¿cÃ³mo sabe la posiciÃ³n de cada una? La respuesta es la **CodificaciÃ³n Posicional**. Antes de que los datos entren al codificador o decodificador, inyectamos una pequeÃ±a pieza de informaciÃ³n matemÃ¡tica (usando funciones seno y coseno) a cada palabra para indicar su posiciÃ³n en la secuencia.

---

## ğŸš€ Â¿CÃ³mo Usar el Proyecto?

Sigue estos pasos para entrenar tu propio modelo o usar el que ya estÃ¡ entrenado para traducir.

### 1. Prerrequisitos

AsegÃºrate de tener Python instalado. Luego, instala todas las dependencias necesarias ejecutando:

```bash
pip install -r requirements.txt
```

Esto instalarÃ¡ PyTorch, Spacy, y otras librerÃ­as necesarias. TambiÃ©n se descargarÃ¡n los modelos de lenguaje de Spacy para espaÃ±ol e inglÃ©s la primera vez que se ejecute el script.

### 2. Entrenar el Modelo

Para entrenar el modelo desde cero, ejecuta el siguiente comando en tu terminal:

```bash
python main.py --mode train --epochs 10 --batch_size 128
```

* `--mode train`: Indica que queremos entrenar el modelo.
* `--epochs 10`: NÃºmero de veces que el modelo verÃ¡ el dataset completo.
* `--batch_size 128`: NÃºmero de oraciones que se procesan en cada paso.

El script descargarÃ¡ automÃ¡ticamente el dataset de traducciÃ³n (un conjunto de pares de oraciones espaÃ±ol-inglÃ©s), lo preprocesarÃ¡ y comenzarÃ¡ el entrenamiento. Los pesos del mejor modelo se guardarÃ¡n en la carpeta `saved_models/` y los vocabularios en `saved_models/vocab.pkl`.

### 3. Traducir una OraciÃ³n (Inferencia)

Una vez que el modelo ha sido entrenado (o si ya tienes un modelo guardado), puedes usarlo para traducir.

```bash
python main.py --mode infer --sentence "una niÃ±a estÃ¡ comiendo una manzana"
```

* `--mode infer`: Indica que queremos usar el modelo para traducir.
* `--sentence "..."`: La oraciÃ³n en espaÃ±ol que deseas traducir.

El programa cargarÃ¡ el modelo y los vocabularios guardados y te mostrarÃ¡ la traducciÃ³n en la terminal.

---

## ğŸ“‚ Estructura del Proyecto

```
/
|-- ğŸ“‚ model/
|   |-- transformer.py               # Define la arquitectura principal del Transformer.
|   |-- transformer_components.py    # Contiene los bloques de construcciÃ³n (AtenciÃ³n, FF, etc.).
|-- ğŸ“‚ utils/
|   |-- data_preprocessing.py        # Descarga, procesa y prepara los datos.
|-- main.py                          # Script principal para entrenar o ejecutar inferencia.
|-- engine.py                        # Contiene las funciones de entrenamiento, evaluaciÃ³n y traducciÃ³n.
|-- requirements.txt                 # Lista de dependencias del proyecto.
|-- LICENSE                          # Licencia del proyecto (MIT).
```

---

## âš–ï¸ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Consulta el archivo `LICENSE` para mÃ¡s detalles.
